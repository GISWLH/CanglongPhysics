"""Autogenerated hindcast script for CAS-Canglong 2022-2023 retrospective based on run_temp.py workflow."""

from __future__ import annotations

import os
import sys
from pathlib import Path
from datetime import timedelta

import numpy as np
import pandas as pd
import torch
import xarray as xr
from tqdm import tqdm

from timm.models.layers import trunc_normal_, DropPath  # noqa: F401
import torch.nn as nn  # noqa: F401
import torch.nn.functional as F  # noqa: F401
from scipy.special import gamma as gamma_function

sys.path.append(str(Path(__file__).resolve().parent.parent))
import canglong.embed_old as embed_old  # type: ignore
sys.modules['canglong.embed'] = embed_old
import canglong.recovery_old as recovery_old  # type: ignore
sys.modules['canglong.recovery'] = recovery_old
from canglong.earth_position import calculate_position_bias_indices  # noqa: F401
from canglong.shift_window import create_shifted_window_mask, partition_windows, reverse_partition  # noqa: F401
from canglong.embed_old import ImageToPatch2D, ImageToPatch3D, ImageToPatch4D  # noqa: F401
from canglong.recovery import RecoveryImage2D, RecoveryImage3D, RecoveryImage4D  # noqa: F401
from canglong.pad import calculate_padding_3d, calculate_padding_2d  # noqa: F401
from canglong.crop import center_crop_2d, center_crop_3d  # noqa: F401


input_constant = None

USE_AUTOMATIC_MIXED_PRECISION = torch.cuda.is_available()

SURFACE_INPUT_PATH = Path('I:/ERA5_np/input_surface_norm_test_last100.pt')
UPPER_INPUT_PATH = Path('I:/ERA5_np/input_upper_air_norm_test_last100.pt')
MODEL_PATH = Path('F:/model/weather_model_epoch_500.pt')
CONSTANT_PATH = Path(__file__).resolve().parent.parent / 'constant_masks' / 'input_tensor.pt'
CLIMATE_PATH = Path('E:/data/climate_variables_2000_2023_weekly.nc')
OUTPUT_DIR = Path('Z:/Data/temp')

SURFACE_VARIABLES = [
    'large_scale_rain_rate',
    'convective_rain_rate',
    'total_column_cloud_ice_water',
    'total_cloud_cover',
    'top_net_solar_radiation_clear_sky',
    '10m_u_component_of_wind',
    '10m_v_component_of_wind',
    '2m_dewpoint_temperature',
    '2m_temperature',
    'surface_latent_heat_flux',
    'surface_sensible_heat_flux',
    'surface_pressure',
    'volumetric_soil_water_layer',
    'mean_sea_level_pressure',
    'sea_ice_cover',
    'sea_surface_temperature'
]

ORDERED_VAR_STATS = {
    'large_scale_rain_rate': {'mean': 1.10e-05, 'std': 2.55e-05},
    'convective_rain_rate': {'mean': 1.29e-05, 'std': 2.97e-05},
    'total_column_cloud_ice_water': {'mean': 0.022627383, 'std': 0.023428712},
    'total_cloud_cover': {'mean': 0.673692584, 'std': 0.235167906},
    'top_net_solar_radiation_clear_sky': {'mean': 856148.0, 'std': 534222.125},
    '10m_u_component_of_wind': {'mean': -0.068418466, 'std': 4.427545547},
    '10m_v_component_of_wind': {'mean': 0.197138891, 'std': 3.09530735},
    '2m_dewpoint_temperature': {'mean': 274.2094421, 'std': 20.45770073},
    '2m_temperature': {'mean': 278.7841187, 'std': 21.03286934},
    'surface_latent_heat_flux': {'mean': -5410301.5, 'std': 5349063.5},
    'surface_sensible_heat_flux': {'mean': -971651.375, 'std': 2276764.75},
    'surface_pressure': {'mean': 96651.14063, 'std': 9569.695313},
    'volumetric_soil_water_layer': {'mean': 0.34216917, 'std': 0.5484813},
    'mean_sea_level_pressure': {'mean': 100972.3438, 'std': 1191.102417},
    'sea_ice_cover': {'mean': 0.785884917, 'std': 0.914535105},
    'sea_surface_temperature': {'mean': 189.7337189, 'std': 136.1803131}
}

LAT_SIZE = 721
LON_SIZE = 1440
LAT_COORDS = np.linspace(90.0, -90.0, LAT_SIZE, dtype=np.float32)
LON_COORDS = np.linspace(0.0, 360.0 - 360.0 / LON_SIZE, LON_SIZE, dtype=np.float32)

WEEKLY_FREQ = '7D'
BASE_START_DATE = pd.Timestamp('2022-01-29')
INITIAL_HISTORY_WEEKS = 4
FORECAST_WEEKS = 6
M_HR_TO_MM_DAY = 24.0 * 1000.0
MIN_HIST_SAMPLES = 10

CLIMATE = xr.open_dataset(CLIMATE_PATH)
if 'latitude' in CLIMATE.dims:
    CLIMATE = CLIMATE.rename({'latitude': 'lat', 'longitude': 'lon'})
CLIMATE = CLIMATE.sortby('lat', ascending=False)

# Paste the Canglong model definition (from run_temp.py) below this comment before executing.

# End of Paste

def build_stat_arrays() -> tuple[np.ndarray, np.ndarray]:
    means = np.array([ORDERED_VAR_STATS[var]['mean'] for var in SURFACE_VARIABLES], dtype=np.float32)
    stds = np.array([ORDERED_VAR_STATS[var]['std'] for var in SURFACE_VARIABLES], dtype=np.float32)
    return means.reshape(-1, 1, 1, 1), stds.reshape(-1, 1, 1, 1)


def load_inputs() -> tuple[np.ndarray, np.ndarray]:
    surface = torch.load(SURFACE_INPUT_PATH, map_location='cpu').float().numpy()
    upper = torch.load(UPPER_INPUT_PATH, map_location='cpu').float().numpy()
    if surface.shape[0] != len(SURFACE_VARIABLES):
        raise ValueError(f"Unexpected surface feature dimension: {surface.shape}")
    return surface, upper


def ensure_device_tensor(tensor: torch.Tensor, device: torch.device) -> torch.Tensor:
    return tensor.to(device) if tensor.device != device else tensor


def load_model(device: torch.device):
    global input_constant
    input_constant = torch.load(CONSTANT_PATH, map_location=device)
    input_constant = ensure_device_tensor(input_constant, device)
    model_obj = torch.load(MODEL_PATH, map_location=device, weights_only=False)
    model = model_obj.module if isinstance(model_obj, torch.nn.parallel.DataParallel) else model_obj
    model.input_constant = input_constant
    model.to(device)
    model.eval()
    return model


def denormalize_surface_block(surface_block: np.ndarray) -> np.ndarray:
    means, stds = build_stat_arrays()
    return surface_block * stds + means


def compute_total_precip_block(surface_block: np.ndarray) -> np.ndarray:
    lsrr = surface_block[:, 0, :, :]
    crr = surface_block[:, 1, :, :]
    return (lsrr + crr) * M_HR_TO_MM_DAY


def compute_pet_block(surface_block: np.ndarray) -> np.ndarray:
    t2m_c = surface_block[:, 8, :, :] - 273.15
    d2m_c = surface_block[:, 7, :, :] - 273.15
    es = 0.618 * np.exp(17.27 * t2m_c / (t2m_c + 237.3))
    ea = 0.618 * np.exp(17.27 * d2m_c / (d2m_c + 237.3))
    ratio = np.where(es > 1e-9, ea / es, 0.0)
    ratio = np.clip(ratio, 0.0, 1.0)
    pet = 4.5 * np.power((1 + t2m_c / 25.0), 2) * (1 - ratio)
    pet = np.maximum(pet, 0.0)
    return pet


def get_week_of_year(dates: xr.DataArray) -> xr.DataArray:
    day_of_year = dates.dt.dayofyear
    return ((day_of_year - 1) // 7) + 1


def _spei_from_combined(D: xr.DataArray, history_len: int) -> xr.DataArray:
    D_hist = CLIMATE['tp'] - CLIMATE['pet']
    hist_week_numbers = get_week_of_year(CLIMATE['time'])
    pred_week_numbers = get_week_of_year(D['time'])
    spei_full = xr.full_like(D, np.nan, dtype=np.float32)
    hist_years = np.unique(CLIMATE['time'].dt.year)

    for idx in range(history_len, D.sizes['time']):
        curr_week_num = pred_week_numbers.isel(time=idx).item()
        curr_accum = sum(D.isel(time=idx - j) for j in range(4))

        hist_accum_list = []
        for year in hist_years:
            year_mask = CLIMATE['time'].dt.year == year
            year_data = D_hist.where(year_mask, drop=True)
            year_weeks = hist_week_numbers.where(year_mask, drop=True)
            week_indices = np.where(year_weeks == curr_week_num)[0]
            if len(week_indices) == 0:
                continue
            week_idx = week_indices[0]
            if week_idx < 3:
                continue
            accum = sum(year_data.isel(time=week_idx - j) for j in range(4))
            hist_accum_list.append(accum)

        if not hist_accum_list:
            continue
        hist_accum = xr.concat(hist_accum_list, dim='time')
        if hist_accum.sizes['time'] < MIN_HIST_SAMPLES:
            continue
        if np.isnan(curr_accum).all():
            continue

        spei_map = xr.apply_ufunc(
            calculate_spei_for_pixel,
            hist_accum,
            curr_accum,
            input_core_dims=[['time'], []],
            output_core_dims=[[]],
            exclude_dims=set(('time',)),
            vectorize=True,
            output_dtypes=[float],
            keep_attrs=True
        )
        spei_full[idx, :, :] = spei_map

    return spei_full


def compute_spei_with_history(
    forecast_total: np.ndarray,
    forecast_pet: np.ndarray,
    forecast_times: pd.DatetimeIndex,
    history_total: np.ndarray,
    history_pet: np.ndarray,
    history_times: pd.DatetimeIndex,
) -> xr.DataArray:
    combined_total = np.concatenate([history_total, forecast_total], axis=0)
    combined_pet = np.concatenate([history_pet, forecast_pet], axis=0)
    combined_times = history_times.tolist() + forecast_times.tolist()

    D = xr.DataArray(
        combined_total - combined_pet,
        coords={'time': pd.DatetimeIndex(combined_times), 'lat': LAT_COORDS, 'lon': LON_COORDS},
        dims=('time', 'lat', 'lon')
    )

    spei_full = _spei_from_combined(D, history_len=len(history_times))
    return spei_full.isel(time=slice(len(history_times), None))


def create_forecast_dataset(denorm_surface: np.ndarray, forecast_week_starts: list[pd.Timestamp]) -> xr.Dataset:
    ds = xr.Dataset(coords={'time': pd.DatetimeIndex(forecast_week_starts),
                            'lat': LAT_COORDS,
                            'lon': LON_COORDS})
    for idx, name in enumerate(SURFACE_VARIABLES):
        ds[name] = (('time', 'lat', 'lon'), denorm_surface[:, idx, :, :].astype(np.float32))
    total_precip = compute_total_precip_block(denorm_surface)
    pet = compute_pet_block(denorm_surface)
    ds['total_precipitation'] = (('time', 'lat', 'lon'), total_precip.astype(np.float32))
    ds['potential_evapotranspiration'] = (('time', 'lat', 'lon'), pet.astype(np.float32))
    ds.attrs['forecast_start_date'] = forecast_week_starts[0].strftime('%Y-%m-%d')
    ds.attrs['forecast_end_date'] = (forecast_week_starts[-1] + timedelta(days=6)).strftime('%Y-%m-%d')
    ds.attrs['prediction_type'] = f'{FORECAST_WEEKS}-Week Forecast'
    return ds


def run_hindcast() -> None:
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    print(f'Using device: {device}')

    surface_norm, upper_norm = load_inputs()
    means, stds = build_stat_arrays()

    observed_denorm = denormalize_surface_block(surface_norm)
    observed_denorm = np.transpose(observed_denorm, (1, 0, 2, 3))
    observed_total = compute_total_precip_block(observed_denorm)
    observed_pet = compute_pet_block(observed_denorm)

    model = load_model(device)

    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    lead1_predictions: list[np.ndarray] = []
    lead1_times: list[pd.Timestamp] = []

    num_weeks = surface_norm.shape[1]

    for idx in tqdm(range(INITIAL_HISTORY_WEEKS, num_weeks), desc='Hindcast weeks'):
        latest_week_idx = idx - 1
        current_week_start = BASE_START_DATE + pd.Timedelta(weeks=latest_week_idx)

        surface_slice = surface_norm[:, idx-2:idx, :, :]
        upper_slice = upper_norm[:, :, idx-2:idx, :, :]

        current_surface = torch.from_numpy(surface_slice).unsqueeze(0).to(device)
        current_upper = torch.from_numpy(upper_slice).unsqueeze(0).to(device)
        if USE_AUTOMATIC_MIXED_PRECISION:
            current_surface = current_surface.half()
            current_upper = current_upper.half()

        surfaces = []
        for lead in range(FORECAST_WEEKS):
            with torch.cuda.amp.autocast(enabled=USE_AUTOMATIC_MIXED_PRECISION):
                output_surface, output_upper = model(current_surface, current_upper)
            surfaces.append(output_surface[:, :, 0:1, :, :].detach().cpu())
            if lead < FORECAST_WEEKS - 1:
                current_surface = torch.cat([current_surface[:, :, 1:2, :, :], output_surface[:, :, 0:1, :, :]], dim=2)
                current_upper = torch.cat([current_upper[:, :, :, 1:2, :, :], output_upper[:, :, :, 0:1, :, :]], dim=3)
                if USE_AUTOMATIC_MIXED_PRECISION:
                    current_surface = current_surface.half()
                    current_upper = current_upper.half()
            del output_surface, output_upper
            torch.cuda.empty_cache()

        surfaces_tensor = torch.cat(surfaces, dim=2).numpy()[0]
        denorm = surfaces_tensor * stds + means
        denorm = np.transpose(denorm, (1, 0, 2, 3))

        forecast_week_starts = [current_week_start + pd.Timedelta(weeks=lead + 1) for lead in range(FORECAST_WEEKS)]
        forecast_ds = create_forecast_dataset(denorm, forecast_week_starts)

        forecast_start = forecast_week_starts[0]
        forecast_end = forecast_week_starts[-1] + timedelta(days=6)
        forecast_file = OUTPUT_DIR / f'canglong_6weeks_{forecast_start.strftime("%Y-%m-%d")}_{forecast_end.strftime("%Y-%m-%d")}.nc'
        forecast_ds.to_netcdf(forecast_file)

        history_indices = range(idx - INITIAL_HISTORY_WEEKS, idx)
        history_total = observed_total[history_indices, :, :]
        history_pet = observed_pet[history_indices, :, :]
        history_times = pd.date_range(
            BASE_START_DATE + pd.Timedelta(weeks=history_indices.start),
            periods=INITIAL_HISTORY_WEEKS,
            freq=WEEKLY_FREQ
        )

        forecast_total = compute_total_precip_block(denorm)
        forecast_pet = compute_pet_block(denorm)
        spei_da = compute_spei_with_history(
            forecast_total,
            forecast_pet,
            pd.DatetimeIndex(forecast_week_starts),
            history_total,
            history_pet,
            history_times
        )

        spei_ds = xr.Dataset(
            {'spei': (('time', 'lat', 'lon'), spei_da.astype(np.float32))},
            coords={'time': pd.DatetimeIndex(forecast_week_starts), 'lat': LAT_COORDS, 'lon': LON_COORDS}
        )
        spei_ds.attrs['forecast_start_date'] = forecast_start.strftime('%Y-%m-%d')
        spei_ds.attrs['forecast_end_date'] = forecast_end.strftime('%Y-%m-%d')
        spei_file = OUTPUT_DIR / f'spei1_forecast_{forecast_start.strftime("%Y-%m-%d")}_{forecast_end.strftime("%Y-%m-%d")}.nc'
        spei_ds.to_netcdf(spei_file)

        lead1_predictions.append(denorm[0])
        lead1_times.append(forecast_week_starts[0])

    lead1_array = np.stack(lead1_predictions, axis=0)
    lead1_total = compute_total_precip_block(lead1_array)
    lead1_pet = compute_pet_block(lead1_array)

    agg_history_total = observed_total[:INITIAL_HISTORY_WEEKS, :, :]
    agg_history_pet = observed_pet[:INITIAL_HISTORY_WEEKS, :, :]
    agg_history_times = pd.date_range(BASE_START_DATE, periods=INITIAL_HISTORY_WEEKS, freq=WEEKLY_FREQ)

    agg_spei = compute_spei_with_history(
        lead1_total,
        lead1_pet,
        pd.DatetimeIndex(lead1_times),
        agg_history_total,
        agg_history_pet,
        agg_history_times
    )

    agg_ds = xr.Dataset(coords={'time': pd.DatetimeIndex(lead1_times), 'lat': LAT_COORDS, 'lon': LON_COORDS})
    for idx_var, name in enumerate(SURFACE_VARIABLES):
        agg_ds[name] = (('time', 'lat', 'lon'), lead1_array[:, idx_var, :, :].astype(np.float32))
    agg_ds['total_precipitation'] = (('time', 'lat', 'lon'), lead1_total.astype(np.float32))
    agg_ds['potential_evapotranspiration'] = (('time', 'lat', 'lon'), lead1_pet.astype(np.float32))
    agg_ds['spei'] = (('time', 'lat', 'lon'), agg_spei.astype(np.float32))
    agg_ds.attrs['hindcast_start_date'] = lead1_times[0].strftime('%Y-%m-%d')
    agg_ds.attrs['hindcast_end_date'] = (lead1_times[-1] + pd.Timedelta(days=6)).strftime('%Y-%m-%d')

    agg_file = OUTPUT_DIR / f'canglong_hindcast_surface_{lead1_times[0].strftime("%Y-%m-%d")}_{(lead1_times[-1] + pd.Timedelta(days=6)).strftime("%Y-%m-%d")}.nc'
    agg_ds.to_netcdf(agg_file)
    print(f'Saved aggregated hindcast to {agg_file}')


if __name__ == '__main__':
    run_hindcast()

